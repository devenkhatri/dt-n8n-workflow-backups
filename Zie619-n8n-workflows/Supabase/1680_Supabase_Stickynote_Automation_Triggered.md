# Workflow Analysis for A/B Split Testing

## Description
This workflow is designed for A/B split testing of Large Language Model (LLM) prompts in a production environment. It randomly assigns chat sessions to either a baseline or an alternative prompt, ensuring the same prompt is used for all interactions within that session. This allows for testing different LLM settings and measuring their efficacy.

## Input Details
The workflow is triggered when a chat message is received, providing the chat input and session ID.

## Process Summary
Upon receiving a chat message, the workflow first defines baseline and alternative prompt values. It then checks a Supabase table to determine if the chat session already exists. If it's a new session, the workflow randomly assigns either the baseline or the alternative path to the session and records this in Supabase. The workflow retrieves the correct prompt (baseline or alternative) based on the session's assigned path. Finally, an AI agent utilizes an OpenAI chat model with the chosen prompt and the received chat input, and the conversation history is stored in a PostgreSQL database.

## Output Details
The workflow produces an AI agent's response generated by the OpenAI chat model and stores the chat conversation history in a PostgreSQL database.

## Tags
A/B Testing, LLM, AI, Chatbot, Supabase, OpenAI, PostgreSQL, Automation, n8n, Production-ready
